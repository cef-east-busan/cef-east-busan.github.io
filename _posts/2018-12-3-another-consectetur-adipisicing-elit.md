---
layout: post
title: "Another consectetur adipisicing elit"
date: 2018-12-03 8:14:30 +0600
tags: 파티전도 무료 강습회
category: 새소식반
post_image: https://cdn.stocksnap.io/img-thumbs/960w/771S335ARZ.jpg
comment: true
excerpt:  Lacinia wisi feugiat tellus neque dui pellentesque, libero Lorem ipsum dolor sit amet, consectetur adipisicing elit.
---
**Please consider re-evaluating the grades given for the following LOs and HCs.**

**Capstone LO’s**

While it was my mistake for not tagging the four CP194 LOs, I don’t believe the grading of all 2’s were justified and I believe my product speaks for itself in these categories.

**#curation** - I believe that the feedback I got is mostly addressed in my product, specifically the first chapter of my series. In the rubric, it simply asks me to "select, organize, and present essential content for an intended purpose". For a 3, I have to do this and provide justification. 

Below  is a screenshot of a section in my first chapter. I prepare the reader for the next chapters to come and justify why I have organized the series in such a way. Then, I present the content that was promised in a well-structured Medium article format where it is also easy to navigate between the six chapters thanks to a hyperlinked table of contents. 

![image alt text](image_0.png)

My product organized the large field of recommender systems and strategically broke it down into 6 parts: intro, evaluation metrics, collaborative filtering, content based, knowledge based, and more advanced recommenders. This gives a great overview of the field (as collaborative filtering, content based, and knowledge based are the main three approaches and others are a combination or extension of the three) and explores intro, ways to evaluate and more advanced topics. For my intended audience, or undergraduate CS students, my series is carefully curated so that they may easily approach this initially daunting field.

The feedback mentions that I did not include justification for topics I left out. However, I didn’t find it necessary and the rubric didn’t ask that of me, nor did my advisors. Adding reasons for why I didn’t cover other relevant topics seems to be (1) a 4 implementation of the LO and (2) something that would muddle my paper if I were to include it in the series.

**#navigation** - This LO requires me to strategically meet commitments and accomplish the aims of the project by relevant deadlines. The feedback I received doesn’t seem very relevant to this criteria. In the **beginning of the spring semester,** I was asked to deliver a certain level of completeness of my product so that my advisors and other higher ups could have confidence that I could finish the project in time (and continue CP194). I delivered and my advisors seemed happy if not impressed. I then continued adding content to my series and received very positive feedback during my **Capstone Defense** from both of my advisors. The two main feedback I received from my defense is to (1) add more chapters and (2) expand on my HC/LO tagging. I have done both for my final product and while I admit my LO tagging sometimes were thin and specific justifications weren’t made, I believe that my work and effort deserves a 3 and referring to specific details and a fuller tagging would justify a 4. I added two more chapters since the defense and fleshed out my HC tagging and some of the LO’s. In general, I have been meeting commitments and accomplishing what was asked of me by the given deadlines.

**#outcomeanalysis** - This LO asks of me to "identify and utilize appropriate measures and rubrics to develop and evaluate work products." The feedback I got refers to the lack of words in my tagging and the missing capstone LOs. Those are valid points. However, I believe that the LOs and HCs I did tag does evaluate my product in mostly appropriate ways. “Identifying and using appropriate measures” is my tagging of specific HCs and LOs and “evaluating my product” on these is what my tagging entries are supposed to be. While quite thin, I do my job of identifying measures and evaluating my work product based on it. I feel like this deserves a 3 for a job done sufficiently but definitely not a 4 for fully fleshed out entries. 

Just adding more words in my entries to raise a grade when I believe a lot of my product speaks for itself feels unfair. The series has received much attention since its release (adding to their list, subscribing to get my stories through mail, clapping, responding, and following) and one comment below shows a reader’s appreciation. While this isn’t exactly an outcome analysis in the HC/LO tagging sense, it shows a certain level of accomplishment in achieving what I wanted it to.

![image alt text](image_1.png)![image alt text](image_2.png)

**#qualitydeliverables** - This LO seems a bit subjective by definition. "Submit work products with the scope, depth, and rigor **appropriate** to the setting or stage of project." The feedback I got says that while I made a lot of progress, it ‘feels a bit on the thin side for a capstone.’ I did not expect such a comment and this comment wasn’t expanded upon further other than an example of something I could have added. When I met with my advisor for my capstone defense and one more time after, I was told that my Medium series was coming along nicely but that I needed a short intro and conclusion along with a strong appendix. I added my introduction which is my executive summary and my conclusion above and below my main Medium series product. My series goes into the appropriate scope, depth, and rigor for a capstone project. The feedback that my capstone is missing an academic framing that wraps everything and converts it into a piece of academic work comes seemingly out of nowhere as it was never brought up in any of our meetings. Does this mean that I needed a larger introduction and conclusion? I did what was asked and while I definitely agree that my tagging appendix was shorter than I would have desired, everything else is up to the standard that I intended. 

A further explanation on what would make my product a ‘fuller capstone’ (as it has 10000+ words with all that was asked) and whether it was mentioned in previous meetings would justify this grading for me (other than a longer appendix). If not, I think a 3 is well deserved here.

**#projectLO** - both of the gradings I want to dispute are ones that didn’t take my coding into consideration. I emailed my advisor after submission after I noticed that not all my code was in my pdf submission. I used gists for the code snippets throughout the articles and that didn’t copy properly. That is my fault for not double checking but I told my advisor that I can show all revision history on both Medium and gists (as that is a feature) to ensure that I didn’t tweak any information on my articles or code after submission. I got a reply saying that they will take a look at it. I assumed that this meant that the code will be taken into consideration when grading the product but the following comments say otherwise. (Perhaps my second advisor graded these and wasn’t informed of what happened).

* **#communication162** - "... not that much code here to gauge by". My code should clearly show well commented, logically flowing lines and clear communication to the reader what is going on. Please take the code into consideration when grading this LO.

* **#neuralnetworks156** - "... I’m not sure where the actual implementation is". My code is on my last article and it didn’t copy over hence the low grade and comment. Please take the code into consideration when grading this LO.

**HCs**

**#evidencebased **- The first half of feedback points out that what I discuss in my tagging is mostly about #sourcequality. This is true and that is indeed what I start off with. I discuss the resources I’m using and their credibility. This is done to show that since the logic from these credible sources are sound and well-accepted, mine (that I directly translate into easier-to-approach language) should too. Of course, the logic of the methods I explain theoretically hold ground by themselves. The second half of feedback states that I should discuss the effectiveness of different methods to better utilize this HC. In my tagging, however, I do expand on how the code I implement "uses evaluation metrics to assess the effectiveness of the algorithms." Additionally, I mention how the same dataset is u